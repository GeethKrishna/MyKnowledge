## The initial phase of keyword selection

1. First we should know what we're optimizing for, for that we should have an idea about what people are typing/speaking into the search engines to search about the kind of products/services that you/your company provide. Those words/phrases are usually called Keywords. **Formal Keyword research** is the process where you do find words that you should optimize for on the basis of **Relevance, Search Volume and Competitiveness**.
2. For start we can brainstorm a list of relevant keywords that you can think of, you can finalize on the list from answering some questions like 1) what do you sell ?, 2) what problem do you solve ?
3. Some tools we can use to find and finalize on these keywords are:
   - Google search console
   - AnswerThePublic
   - Moz Keyword explorer
   - UberSuggest
4. After finding some relevant keywords now we can move to focus on search volume and competitiveness. You may already realize that most of the relevant keywords that you've chosen are typed in thousands or even millions of time daily and these keywords have a lot of competition where we may not succeed. So, we need to find some more keywords/key phrases that are more descriptive, more relevant to our use case and have less competition. These kind of keywords will bring us more intent-driven traffic if we optimized our site for these keywords. These keywords are called **Long tail Keywords**, keep in mind that a single Long tail keyword may not give you the impact that of a highly competitive keyword but a collection of these long tail keywords that are relevant will make your site rank higher from when you've optimized your site to high competitive keywords. This process of grouping is called **Keyword categorization**.
5. We can use **Google Trends** to compare one word with another in different regions and different devices.
6. Now, this next part is called **Keyword Distribution**, this is where we assign certain keywords to certain pages of your website. This makes sure that that keywords in the specific pages are relevant and aligned to content in those pages. Once you've finalized on the keywords that you are trying to optimize for then we can start mapping the keywords to the specific pages where the content of the page and the keyword are relevant and the mapping looks meaningful. if we divide this into steps:
	- First List out your pages in your website
	- Take your list of keywords that you've finalized and match each keyword with each page of your website. It is optimal if each page is optimized for a single keyword, If two or more keywords are quite similar then do not hesitate to match all those keywords to a single page.
	- Keep in your mind that search engines prefer unique and relevant content in a page, so if there's a keyword that doesn't match any of your page's content then create a new page for it.
	- Understand that the Keyword is the important thing here no the pages of it's content, So make sure that the content prepared around the keywords and not the other way around.
	- One of the important thing that most people forget is keeping the **URL** SEO friendly too.
7. If you have doubts about the keywords or the quality of the traffic these keywords bring then you can try to create google ad's for a short time by buying the certain keywords that you want to optimize. Now after analyzing the traffic you can decide to whether to invest on doing SEO for that keyword or to skip the keyword.
8. This process of Formal Keyword Research, Keyword Categorization and Keyword Distribution is a continuous process that we should do a couple of times every year to keep up with the changing trends of the world.

## Content Optimization

1. Let's start with this "Both People and Search engines expect **clarity and quality** form the webpages and their content".
2. For this to happen the Quality of the content should be good and relevant to the keywords and at the same time the page should have a clear agenda about what it wants to tell or share about in every aspect of it like the title, header, images, metadata, etc., in all possible places to have the best ranking and user experience.
3. Another thing we can do to have more clarity is to have a clear and meaningful site/page structure for our website. A good and defined site structure can help search engines to understand where different pages are located and will suggest those pages to users quickly when they search for it.
4. There's no clear and defined site structure to follow, site structure completely depends on the use case and the type of audience that you are trying to target but make sure that the structure makes sense and is easy to understand and navigate to novice users too.
5. **On-page Optimization** is the process of fine-tuning the content of the page to make it more aligned and relevant to keyword that the page is targeting.
6. The first thing on the page we should optimize is the **URL**, make sure that the URL is concise and at the same time relevant to the content of the page that the URL points to.
7. Also, if possible try to include the keyword that the page is targeting/optimized for in the URL.
8. Use '-' instead of other word separators.
9. Now, if we dive into the HTML source code of the page the first thing you'll see is the title tag, make sure that the title is descriptive of the page and is near 65 characters. Keep in mind that this meta-title tag is typically the one that the search engines uses in their SERP listing, so phrase them as best as possible. But some times search engine may display some other title in the SERP to keep the listings more relevant to the search words.
10. Another important tag is the meta-description tag as it is the one typically used by the search engines in the SERP below the title and URL of the page in the listings as the page descriptor.
11. Keep in mind that this meta-description tag is widely ignored by many search engines and will not effect your rankings but this will effect your click-through rates as this is the text that will explain/describe about your content to the users. A loose guideline is to keep this near 156 characters for optimal length as this is what major search engines truncate the descriptions to.
12. The meta-keywords tag looks important but it's not as search engines started to ignore this tag long ago.
13. The h1 tag is also an import tag as this is the headline that is visible to user after coming to your site, keep this descriptive and concise.
14. Now, the most important part the **Content**, Don't go overboard and over-optimize for search engines. Always write for humans first, search engines second. The search engines just like humans look for other relevant information in the page so, make sure you include other relevant keywords and information in the page and also include the keyword that the page targets for a few times but don't go overboard.
15. Just like content in the page can be optimized even the images can be optimized. The search engines are not that evolved yet that they try to understand the image and try to extract text/context from it. So, some things we can do is 1) give a good descriptive _alt_ text 2) make sure the URL of the image is also meaningful just like the URL of the page. One more way we can optimize for non-text elements in our pages is to describe about these elements in the text elements of the page that are surrounding the non-text elements. this will help the search engine understand what is the non-text element and why it is there.
16. This is all we can do in HTML, but if go slightly deeper we can use some structured data to provide more specific meta-data to specific non-text elements in our page. One famous format preferred by search engines is using schema.org markup in JSON-LD. In this JSON object we can provide all kinds of meta-data to our video and image tags which will help our photo our video to come up in the search results. If we add this we can check it's effect in Schema Markup Validator or Google Rich Results to know to extend of effectiveness of this meta-data.
17. We can go down this rabbit hole of optimizing all the tags and code of our website but a good limit is to optimize the URL, Title, Description, Headers/Headlines, Body, Text/Content, Images and Videos. 
18. Search engines discover new content or new pages by following links, site maps, ping services and some little help from us by setting some rules for them so that it becomes easy for them.
19. One of the easiest way to make your site discoverable to search engine bots/crawlers is to make sure that links are pointing to that site. This can be done from menu's, promotional content, linking this from existing pages, etc., way to link this new page form other pages to increase the number of backlinks of the page to increase it's discoverability. 
20. Another clear and structured way of doing this is to use XML sitemaps to tell the search engine bots how many and where does the all the remaining pages of the website are there.
21. A site map is just an organized list of all the pages in our website listed in one place mirroring our real website structure written in XML. Many pages will have an HTML version of this in their Footer of every page in the website. But there's a way to feed this information directly to the search engine and their crawlers through XML. For syntax and more information - https://sitemaps.org 
22. Once the site map is ready we can submit this directly to the search engines itself by going to your google search console for your domain.
23. To go another step further, we can set some rules for the crawlers to follow in `robots.txt` file. This can be used maybe to stop the crawlers from going into some pages like testing pages or member only pages, this can set in the robot.txt file and the crawler will not explore those pages and so these pages will not come up on search results. more on - https:/robotstxt.org 
24. But this will not stop these pages from coming in SERP's entirely. To do so it is recommended to use a noindex-meta tag is preferred. So `robots.txt` file just controls how a site is crawled  and to stop the site from completely coming up from SERP's you should use noindex-meta tag: `<meta name="robots" context="noindex">`
25. One thing that effects all the rankings is server uptime and page load speeds, these will effect both ranking of the page and also the user experience. So, plan so that the page loads as fast as possible.
26. And also make sure that the website is secured with HTTPS, this not only gives more security and trust to your users but also if not used your page will not be coming up in SERP's as search engines straight up do not trust plain HTTP pages.
27. Search engines rely on unique URL's as pointers to content on the web. But often various URL's may point to the same piece of content or same page. This happens because there maybe query parameters on the URL that do not effect the content but are crucial like sessions id's or tracking id's which introduces the problem called **Duplicate Content**.
28. Some times the query params may really decide the content that shows up on the page but some times the content is constant for all the query parameters. In such cases to make sure that search engine does not confuse we use rel-cannonical meta tag, this tells the search engine that no matter what URL is being pointed to the content only consider the main URL component that the tag defines. Another way is to tell the search engine directly in google search console which query parameters to ignore.
29. And if you've moved some content from one URL to another then following proper re-directing rules is necessary. One way to do is using HTTP status codes. A 302 redirect is used when the content is temporarily shifted to another place and will come up shortly so telling the search engine to not to change the indexing/ranking of the page or URL. And use 301 status code if the content is permanently shifted to the redirected page, so the search engine will now transfer every thing it knows about the old URL to the new URL making this new URL the one to come up in SERP instead of the old one.
30. Schema defined the structure and attributes of all kinds of elements. https://schema.org is an universally shared vocabulary of all the different elements actually founded by the search engine organizations. There are many elements and their structures to follow like recipe, book, hackathon, etc., we can find all of these on the website. there are many ways to do this but recommended method is to use JSON-LD (JSON for linked data).
31. Also core web vitals are also used by these search engines to rate the pages for ranking and some main vitals are
	1. Largest Contentful Paint (LCP) - it is amount of time it takes to render the largest content element visible on the screen
	2. First Input Delay (FIP): it is the time from when a user first interacts with the page through click, tap or something like to when the browser responds.
	3. Cumulative Layout Shift (CLS): measures just how much different components in your layout shift around while users trying to interact.
	So, please keep an eye on these web vitals when trying to optimize your page, as this is also as important or even more important than keyword specific optimization.

## Long term plans

1. To have long term plans first the existing site structures, meta tags, and even the content should be good and should support future addition of more pages and optimizations.
2. But ultimately the content is the king and the hardest part is to continuously manage, update and optimize the content in a sustainable way.
3. For this to happen it is very important have a clear **Content strategy** by keeping the Defined business goals and any KPI's that you are measuring.
4. A tool that can be used is called **Content Calendar** which can be used to track the content, when it should be implemented, who should implement it, location of it, etc., information that you may need in a spread sheet and keep of track of all the content in a structured manner.
5. Keep up with trends, news, social media, your competitor to know about new information or content that you can implement for more traffic or more quality in that traffic.
6. And finally do not stop measuring your KPI's for your pages, that is what will help you decide to whether push the content with more promotions or to change the content.

## Links

1. One of most important aspects of SEO is **Links**, specifically **Backlinks**, Cause search engines think each link as a *vote of confidence* for what ever the link points to. The more the number of backlinks (links that point to this page) the more the search engine trusts the page i.e., **authority**. more about it in - [[Basic]] 4th point.
2. So, one minor way to optimize this Linking is to have a anchor tag that is descriptive of the page you point to rather than having anchor texts be unrelated like "click me", "visit", etc., this tip if followed will give the search engine some context and will give the linking more credibility.
3. One major thing to keep in mind is to not to try to trick the system by paying some third party organizations to get monthly x number of links to your page to increase your rankings. If found the penalty can be dropping you from the ranking or even blocking your domain by flagging you as spam or fraud.
4. There are 2 types of links here:
	1. Internal linking: this is the links that you do inside your own site to external other sites or your other pages, this will help the search engine to understand the theme, content, the connection of you content to all the other content in the web. the links that you do when using other site's information or quoting other site's is call **Contextual linking**. 

## Measuring your SEO

1. First make sure that a web analytics service is linked to your page, it can be any like google analytics or adobe analytics and configure it properly.
2. Define proper KPI's that you can use to measure your SEO success, they can be anything like amount of traffic to the site, the number of new signups or even downloads. Decide on any few KPI's that you feel are important to your business.
3. And not only the web analytics we should also an eye on you web vital so that we can make sure that the website performance is not the reason for any decline in our KPI's.
4. Also keep checking the rankings of your chosen keywords that you've optimized your page for, if any any keyword has gone off the trend do not hesitate to focus on a new keyword that is in trend and to change your content around it.
5. Use AI chatbots for measuring and helping you in this SEO journey.
